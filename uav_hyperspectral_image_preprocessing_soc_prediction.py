# -*- coding: utf-8 -*-
"""UAV_Hyperspectral_Image_Preprocessing_SOC_Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1To7zstvbIlF4R1UpVPnYOvxUTVWOiI4A
"""

from osgeo import gdal as GD
import matplotlib.pyplot as plt
import numpy as np

data_set = GD.Open(r'D:\Data\ENVI_Mosaic5_cl.tif')
print(data_set.RasterCount)

# Store the red, green, and blue bands in separate variables
band_1 = data_set.GetRasterBand(108)  # red channel
band_2 = data_set.GetRasterBand(68)   # green channel
band_3 = data_set.GetRasterBand(32)   # blue channel

b1 = band_1.ReadAsArray()
b2 = band_2.ReadAsArray()
b3 = band_3.ReadAsArray()

img_1 = np.dstack((b1, b2, b3))

# Adjust contrast by specifying vmin and vmax
vmin = img_1.min()  # minimum pixel value
vmax = img_1.max()  # maximum pixel value
print(vmin,vmax)
f = plt.figure(facecolor='white')
plt.imshow(img_1, vmin=vmin, vmax=vmax)
plt.savefig('FullImageTiff.png')
plt.show()

import numpy as np

def Count_NonNanElements(data):
    # Count the number of non-NaN elements in the input data
    count = np.count_nonzero(~np.isnan(data))
    return count


print(Count_NonNanElements(b1))
print(Count_NonNanElements(b2))
print(Count_NonNanElements(b3))

width = data_set.RasterXSize
height = data_set.RasterYSize
print("Width:", width)
print("Height:", height)
num_pixels=width*height
print(num_pixels)

# Create folders for saving PNG and CSV files
import os
png_folder = 'D:\\Tiles_Image'
# os.makedirs(png_folder, exist_ok=True)

csv_folder = 'D:\Tiles_CSV'
# os.makedirs(csv_folder, exist_ok=True)

import numpy as np
from PIL import Image
from osgeo import gdal
import matplotlib.pyplot as plt

# Open the original TIFF file
dataset = gdal.Open('D:\Data\ENVI_Mosaic5_cl.tif')

# Get the width and height of the image
width = dataset.RasterXSize
height = dataset.RasterYSize

# Define the tile size
tile_width = width // 12  # Each row will have 10 tiles
tile_height = height // 5  # Each column will have 6 tiles

# Iterate over the rows and columns to create tiles
for row in range(5):
    for col in range(12):
        # Calculate the tile boundaries
        left = col * tile_width
        upper = row * tile_height
        right = left + tile_width
        lower = upper + tile_height

        # Create an empty list to store the band data for the tile
        band_data = []

        # Read the appropriate bands for the tile
        band_1 = dataset.GetRasterBand(108)  # red channel
        band_2 = dataset.GetRasterBand(68)  # green channel
        band_3 = dataset.GetRasterBand(32)  # blue channel
        b1 = band_1.ReadAsArray(left, upper, tile_width, tile_height)
        b2 = band_2.ReadAsArray(left, upper, tile_width, tile_height)
        b3 = band_3.ReadAsArray(left, upper, tile_width, tile_height)
        img_1 = np.dstack((b1, b2, b3))

        # Adjust contrast by specifying vmin and vmax
        vmin = img_1.min()  # minimum pixel value
        vmax = img_1.max()  # maximum pixel value
        print(vmin,vmax)
#         # Create a PIL Image object
#         image = Image.fromarray(img_1)
        f = plt.figure()
        plt.imshow(img_1, vmin=vmin, vmax=vmax)


        # Add the tile number as text annotation
        plt.text(0, 20, f'Tile {row}_{col}', color='white', fontsize=12, fontweight='bold')

        # Save the figure with a unique filename for each tile
        filename_png = os.path.join(png_folder, f'tile_{row}_{col}.png')
        plt.savefig(filename_png)

# Close the original dataset
dataset = None

import os
import numpy as np
from osgeo import gdal
import pandas as pd
# Open the original TIFF file
dataset = gdal.Open('D:\Data\ENVI_Mosaic5_cl.tif')

# Get the width and height of the image
width = dataset.RasterXSize
height = dataset.RasterYSize

# Define the tile size
tile_width = width // 12  # Each row will have 12 tiles
tile_height = height // 5  # Each column will have 5 tiles


# Iterate over the rows and columns to create tiles
for row in range(5):
    for col in range(12):
        # Calculate the tile boundaries
        left = col * tile_width
        upper = row * tile_height
        right = left + tile_width
        lower = upper + tile_height

        # Read all the bands for the tile
        tile_data = []
        for band_index in range(dataset.RasterCount):
            band_data = dataset.GetRasterBand(band_index + 1).ReadAsArray(left, upper, tile_width, tile_height)
            if band_data is not None:
                band_data = np.nan_to_num(band_data, nan=0.0)  # Replace NaN values with zero
                tile_data.append(band_data)
                prev_band_shape=band_data.shape
            else:
                if prev_band_shape is not None:
                    tile_data.append(np.zeros(prev_band_shape))  # Append zero-filled array with previous shape

        # Convert the tile data list to a NumPy array
        tile_data_array = np.array(tile_data)

        # Reshape the tile data to have shape (-1, 269)
        tile_data_reshaped = tile_data_array.reshape(-1, dataset.RasterCount)

        # Create the column names for the CSV file
        column_names = [f"Band No {band_index + 1}" for band_index in range(dataset.RasterCount)]

        # Save the tile data as a CSV file
        filename_csv = os.path.join(csv_folder, f'tile_{row}_{col}.csv')
        np.savetxt(filename_csv, tile_data_reshaped, delimiter=',', fmt='%.8f', header=','.join(column_names),comments='')


# Close the original dataset
dataset = None

# Create folders for saving PNG and CSV files
import os
ndvi_csv_folder = r'D:\Shagun\NDVI_Results_CSV'
# os.makedirs(ndvi_csv_folder, exist_ok=True)

# import numpy as np
# import pandas as pd
# import glob
# import os
# import matplotlib.pyplot as plt

# # Get a list of file paths for the CSV files
# csv_files = glob.glob('D:\Shagun\Tiles_CSV\*.csv')  # Replace with the appropriate file path and pattern

# # Create folders for saving PNG and CSV files
# ndvi_csv_folder = 'D:\Shagun\\NDVI_Results_CSV'
# os.makedirs(ndvi_csv_folder, exist_ok=True)

# # Iterate over the CSV files
# for file_path in csv_files:
#     # Load the CSV file containing endmember spectra
#     endmembers_data = pd.read_csv(file_path)

#     # Calculate NDVI using the appropriate bands
#     red_band = endmembers_data['Band No 107']  # Replace 'Red' with the actual band name or index
#     nir_band = endmembers_data['Band No 223']  # Replace 'NIR' with the actual band name or index

#     ndvi = (nir_band - red_band) / (nir_band + red_band)

#     # Set the threshold value for NDVI to classify vegetation
#     ndvi_threshold = 0.26

#     # Create a boolean mask to identify vegetation pixels
#     vegetation_mask = ndvi > ndvi_threshold

#     # Filter the endmembers to exclude vegetation pixels
#     soil_pixels = endmembers_data[~vegetation_mask]

#     # Get the base file name without extension
#     base_file_name = os.path.splitext(os.path.basename(file_path))[0]

#     # Construct the result file path with the base file name appended with "_NDVIResult"
#     result_file_path = os.path.join(ndvi_csv_folder, base_file_name + '_NDVIResult.csv')

#     # Drop rows where all columns have 0 values
#     soil_pixels = soil_pixels.loc[~(soil_pixels == 0).all(axis=1)]

#     # Save the filtered DataFrame to a new CSV file
#     soil_pixels.to_csv(result_file_path, index=False)
#     print(soil_pixels.shape)
#     print("Processed:", file_path, "-> Saved as:", result_file_path)

################## New NDVI Results ########################################
import numpy as np
import pandas as pd
import glob
import os
import matplotlib.pyplot as plt

# Get a list of file paths for the CSV files
csv_files = glob.glob('D:\Shagun\Tiles_CSV\*.csv')  # Replace with the appropriate file path and pattern

# Create folders for saving PNG and CSV files
new_ndvi_csv_folder = 'D:\Shagun\\New_NDVI_CSV'
counter=0

# Iterate over the CSV files
for file_path in csv_files:
    # Load the CSV file containing endmember spectra
    endmembers_data = pd.read_csv(file_path)

    counter += 1
    # Calculate NDVI using the appropriate bands
    red_band = endmembers_data['Band No 107']  # Replace 'Red' with the actual band name or index
    nir_band = endmembers_data['Band No 223']  # Replace 'NIR' with the actual band name or index

    ndvi = (nir_band - red_band) / (nir_band + red_band)

    # Set the threshold value for NDVI to classify vegetation
    ndvi_threshold = 0.26

    # Create a boolean mask to identify vegetation pixels
    vegetation_mask = ndvi > ndvi_threshold

    # Filter the endmembers to exclude vegetation pixels
    soil_pixels = endmembers_data[~vegetation_mask]

    # Get the base file name without extension
    base_file_name = os.path.splitext(os.path.basename(file_path))[0]

    # Construct the result file path with the base file name appended with "_NDVIResult"
    result_file_path = os.path.join(new_ndvi_csv_folder, base_file_name + '_NDVIResult.csv')

    # Drop rows where all columns have 0 values
    soil_pixels = soil_pixels.loc[~(soil_pixels == 0).all(axis=1)]

    if counter==1:
        Base_soil_pixel_data=soil_pixels

    # Generate random values for Missing data
    if soil_pixels.shape[0] == 0:
       # Select a portion of data from the source DataFrame using random sampling
        portion_size = 60000  # Specify the desired portion size
        portion = Base_soil_pixel_data.sample(n=portion_size)  # Randomly select 'portion_size' rows from the source DataFrame
        # Copy the portion of data into the target DataFrame
        soil_pixels = portion.copy()

    # Save the filtered DataFrame to a new CSV file
    soil_pixels.to_csv(result_file_path, index=False)
    print(soil_pixels.shape)
    print("Processed:", file_path, "-> Saved as:", result_file_path)

################ Function to plot spectra ###########################
BeforeMSCSpectra='D:\Shagun\Soil_Spectra_Before_MSC'

def plot_signature(df,filename):
    plt.figure(figsize=(12, 6))
    pixel_no = 99  #np.random.randint(df.shape[0])
    x_values = np.arange(400, 1000, step=(600 / 269))  # Adjust the range of x-values
    y_values = df.iloc[pixel_no, :].values.tolist()
    plt.plot(x_values, y_values)
    plt.legend()
    # Find the index of the underscore before "NDVIResult"
    index = filename.index("_NDVIResult")

    # Extract the substring before "_NDVIResult"
    extracted_filename = filename[:index]

    plt.title(f'Soil spetcra {extracted_filename}) Pixel({pixel_no}) signature', fontsize=14)
    plt.xlabel('Wavelength', fontsize=14)
    plt.ylabel('Reflectance', fontsize=14)
    # Savethe figure with a unique filename for each tile
    filename_png = os.path.join(BeforeMSCSpectra, f'Soil_Spectra_{extracted_filename}.png')

    plt.savefig(filename_png)

    plt.show()

############################ Plotting spectra for each CSV before MSC ########################################
################## New NDVI Results ########################################
import numpy as np
import pandas as pd
import glob
import os
import matplotlib.pyplot as plt

# Get a list of file paths for the CSV files
csv_files = glob.glob('D:\Shagun\\New_NDVI_CSV\*.csv')  # Replace with the appropriate file path and pattern
for file_path in csv_files:
    # Load the CSV file containing endmember spectra
    data = pd.read_csv(file_path)

    # Extract the file name without extension
    file_name = os.path.splitext(os.path.basename(file_path))[0]

    plot_signature(data,file_name)

import numpy as np
import pandas as pd
from scipy.io import savemat
from scipy.ndimage import uniform_filter1d


# Reading Data
def read_data(df):
    # Convert the dataframe to a numpy array
    data_array = df.values
    return data_array

# Mean Center the data
def mean_centering(input_data):
    output_data = np.empty_like(input_data)  # Initialize output_data as an empty array with the same shape as input_data
    for i in range(input_data.shape[0]):
        output_data[i, :] = input_data[i, :] - input_data[i, :].mean()
    return output_data


# Choose Reference data transposed (intensities in rows)
def choosing_reference(input_data, reference=None):
    # Get reference spectrum. If not given, estimate it from the mean
    if reference is None:
        # Calculate mean
        ref = np.mean(input_data, axis=0)
    else:
        ref = reference
    return ref


def fit_and_correct(input_data, ref):
    fit = np.polyfit(ref, input_data, 1, full=True)
    data_msc = (input_data - fit[0][1]) / fit[0][0]

    # Apply Offset Correction (OC) to ensure non-negativity
    offset = np.abs(np.min(data_msc)) + 0.01  # Add a small constant offset
    data_msc_positive = data_msc + offset

    # Apply moving average smoothing
    data_msc_smoothed = uniform_filter1d(data_msc_positive, size=5)

    return data_msc_smoothed



# Loop MSC
def loop_msc(input_data_group):
    output_group = []
    for input_data in input_data_group:
        msc_data = fit_and_correct(input_data, ref)
        output_group.append(msc_data)
    return output_group

# Saving Data
def write_msc_output(filename,ref):
    savemat(f'D:\Shagun\MSC_mat_results\\{filename}', {"img": ref})

# Iterate over the CSV files
for file_path in csv_files:
    # Load the CSV file containing endmember spectra
    dataf = pd.read_csv(file_path)
    # Extract the file name without extension
    file_name = os.path.splitext(os.path.basename(file_path))[0]

    input_data = read_data(dataf)
    output_data = mean_centering(input_data)
    ref = choosing_reference(input_data, reference=None)
    msc_data = loop_msc(input_data)
#     type(msc_data)
    msc_data_array = np.array(msc_data)
    write_msc_output(file_name,msc_data_array)
    count = len(msc_data)
    print()
    print('SUCCESS')
    print(str(count) + ' spectra transformed with MSC and smoothed')

################ Function to plot spectra ###########################
AfterMSCSpectra='D:\Shagun\Soil_Spectra_After_MSC'

def plot_signature(df,filename):
    # Find the index of the underscore before "NDVIResult"
    index = filename.index("_NDVIResult")

    # Extract the substring before "_NDVIResult"
    extracted_filename = filename[:index]
    plt.figure(figsize=(12, 6))
    pixel_no = np.random.randint(df.shape[0])
    x_values = np.arange(400, 1000, step=(600 / 269))  # Adjust the range of x-values
    y_values = df.iloc[pixel_no, :].values.tolist()
    plt.plot(x_values, y_values)
    plt.legend()
    plt.title(f'After MSC Soil spectra {extracted_filename} Pixel({pixel_no}) signature', fontsize=14)
    plt.xlabel('Wavelength', fontsize=14)
    plt.ylabel('Reflectance', fontsize=14)
    # Savethe figure with a unique filename for each tile
    filename_png = os.path.join(AfterMSCSpectra, f'Soil_Spectra_{extracted_filename}.png')

    plt.savefig(filename_png)

    plt.show()

import glob
from scipy.io import loadmat
import os
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
folder_path = 'D:\Shagun\MSC_mat_results'

file_paths = glob.glob(folder_path + '/*')
# Now you have a list of all file paths in the folder
for file_path in file_paths:


    # Extract the file name without extension
    file_name = os.path.splitext(os.path.basename(file_path))[0]

    ## Read the data.
    mat_contents = loadmat(file_path)

    # Print variable names
    print(mat_contents.keys())
    mat_contents['img'].shape

    # Assuming matcontents['img'] is your dictionary
    msc_dict = mat_contents['img']

    # Convert the dictionary to a dataframe
    df1 = pd.DataFrame.from_dict(msc_dict)

    plot_signature(df1,file_name)
    print("Done Soil Spectra After MSC",file_path)

# Link https://github.com/Laadr/VCA
import numpy as np

def vca(Y, R):
    L, N = Y.shape

    # Center the data
    Y_centered = Y - np.mean(Y, axis=1, keepdims=True)

    # Compute the covariance matrix
    cov_matrix = np.dot(Y_centered, Y_centered.T) / (N - 1)

    # Perform eigendecomposition
    eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)

    # Sort eigenvectors based on eigenvalues
    sorted_indices = np.argsort(eigenvalues)[::-1]
    sorted_eigenvectors = eigenvectors[:, sorted_indices]

    # Select the top R eigenvectors as endmembers
    endmembers = sorted_eigenvectors[:, :R]

    return endmembers # LXN returns where L is the number of Bands and N is no. of Pixels

import numpy as np
import glob
from scipy.io import loadmat
import os
import pandas as pd
import matplotlib.pyplot as plt

folder_path = 'D:\Shagun\MSC_mat_results'
file_paths = glob.glob(folder_path + '/*')
Root_folder_path='D:\Shagun\EndMemberSpectra'

# Create an empty DataFrame to store the endmember values
endmember_df =  pd.DataFrame(columns=np.arange(1, 270))

# Now you have a list of all file paths in the folder
for file_path in file_paths:

    # Extract the file name without extension
    file_name = os.path.splitext(os.path.basename(file_path))[0]

    # Find the index of the underscore before "NDVIResult"
    index = file_name.index("_NDVIResult")

    # Extract the substring before "_NDVIResult"
    extracted_filename = file_name[:index]

    ## Read the data.
    mat_contents = loadmat(file_path)
    mat_contents['img'].shape

    # Assuming matcontents['img'] is your dictionary
    msc_dict = mat_contents['img']

    # Convert the dictionary to a dataframe
    df1 = pd.DataFrame.from_dict(msc_dict)

    #Afer MSC convert df into array
    array = df1.values
    Y= array #M – numpy array 2d matrix of HSI data ((m x n) x p).(num_pixels,num_bands)


    R = 1  # Number of endmembers to extract

    endmembers = vca(Y.T, R)

    # Print the extracted endmembers
    print("Estimated Endmembers:")
    endmembers_Transpose=endmembers.T
    print(endmembers_Transpose.shape)


    #Plot end member for each tile
    plt.figure(figsize=(10, 6))
    x_values = np.arange(400, 1000, step=(600 / 269))  # Adjust the range of x-values
    y_values = endmembers_Transpose[0]   # Extract the values of the row as a list

    plt.plot(x_values,y_values, label=f'Endmember {extracted_filename}')
    plt.legend()
    plt.xlabel('Wavelength')
    plt.ylabel('Reflectance')
    plt.title('Endmembers')
    plt.legend()

    Endmembers_png = os.path.join(Root_folder_path, f'Endmember_Spectra_{extracted_filename}.png')

    plt.savefig(Endmembers_png)
    plt.show()

    # Append the endmember values to the DataFrame
    endmember_df = endmember_df.append(pd.DataFrame(endmembers_Transpose, columns=np.arange(1, 270)), ignore_index=True)

# Save the DataFrame to a CSV file
endmember_df.to_csv('D:\Shagun\Endmember_values.csv', index=False)

import pandas as pd
df=pd.read_csv('D:\Shagun\Endmember_values.csv')
num_negative = (df < 0).sum().sum()
print("Number of Negative Values:", num_negative)

num_zeros = (df == 0).sum().sum()
print("Number of Zero Values:", num_zeros)

num_nan = df.isna().sum().sum()
print("Number of NaN Values:", num_nan)

num_null = df.isnull().sum().sum()
print("Number of Null Values:", num_null)

# Assuming your DataFrame is called 'df'
df = df.applymap(lambda x: abs(x) if x < 0 else x)

num_negative = (df < 0).sum().sum()
print("Number of Negative Values:", num_negative)

# Save the DataFrame to a CSV file
df.to_csv('D:\Shagun\FinalSoilData.csv', index=False)

data=pd.read_csv('D:\Shagun\FinalSoilData.csv')

data.head()

##################### OC ############################################
# Select the first 269 columns and the last output column
OC_df = data.iloc[:, :269].copy()
OC_df['OC'] = data['OC'].copy()
OC_df.head()

X_OC=OC_df.iloc[:,:269]
y_OC=OC_df.iloc[:,-1]

X_OC, y_OC

from sklearn.model_selection import train_test_split

X_OC_train,X_OC_test,y_OC_train,y_OC_test=train_test_split(X_OC,y_OC,test_size=0.3,random_state=42)

print(X_OC_train.shape,X_OC_test.shape)

from sklearn.decomposition import PCA

pca=PCA(n_components=20)
X_OC_train=pca.fit_transform(X_OC_train)
X_OC_test=pca.transform(X_OC_test)
explained_variance_OC=pca.explained_variance_ratio_
print(explained_variance_OC)

########################################## MLP #########################
from sklearn.neural_network import MLPRegressor
nn=MLPRegressor(hidden_layer_sizes=(50,50),activation='relu',max_iter=8500,solver='sgd',learning_rate_init= 0.02, learning_rate= 'adaptive',alpha= 0.1)
nn.fit(X_OC_train,y_OC_train)

# MLP
from sklearn.model_selection import GridSearchCV
param_grid = {
    'hidden_layer_sizes': [(50,60),(40,50),(20,50)],
    'max_iter': [5000,7000,15000],
    'activation': ["identity", "logistic", "tanh", "relu"],
    'solver': ["lbfgs", "sgd", "adam"],
    'alpha': [ 0.05,0.1,1],
    'learning_rate': ['constant','adaptive'],
    'learning_rate_init': [0.05,0.02,0.1],
}

grid = GridSearchCV(nn, param_grid, n_jobs= -1, cv=5)
grid.fit(X_OC_train, y_OC_train)

print(grid.best_params_)

from sklearn import metrics
import math
# For Training Data
rmse1=math.sqrt(metrics.mean_squared_error(y_OC_train,nn.predict(X_OC_train)))
rsq1=metrics.r2_score(y_OC_train,nn.predict(X_OC_train))
print(rmse1,rsq1)
# For Testing Data

rmse2=math.sqrt(metrics.mean_squared_error(y_OC_test,nn.predict(X_OC_test)))
rsq2=metrics.r2_score(y_OC_test,nn.predict(X_OC_test))
print(rmse2,rsq2)

from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, explained_variance_score


# Calculate performance metrics
mse = mean_squared_error(y_OC_test, nn.predict(X_OC_test))

mae = mean_absolute_error(y_OC_test, nn.predict(X_OC_test))

# Print the performance metrics
print("Mean Squared Error (MSE):", mse)

print("Mean Absolute Error (MAE):", mae)

import numpy as np


# Calculate the range of the reference values
range_reference = np.max(y_OC_test) - np.min(y_OC_test)

# Calculate the differences between the predicted and reference values
differences = nn.predict(X_OC_test) - y_OC_test

# Calculate the standard deviation of the differences
std_deviation = np.std(differences)

# Calculate the RPD
rpd = range_reference / std_deviation

# Print the RPD
print("RPD:", rpd)

from sklearn.svm import SVR
from sklearn import *
import math
from sklearn.metrics import r2_score,mean_squared_error
regressor = SVR(kernel = 'sigmoid',C=20,gamma='auto',max_iter=50)
regressor.fit(X_OC_train, y_OC_train)
y_test_fit = regressor.predict(X_OC_test)
r2 = r2_score(y_OC_test, y_test_fit)
print("r2:",r2)
print("RMSE:", math.sqrt(mean_squared_error(y_OC_test, y_test_fit)))

# SVR
from sklearn.model_selection import GridSearchCV
param_grid = {
    'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],
    'C': [10,50,200],
    'gamma': ['scale','auto'],
    'max_iter': [50,500,700],
}

grid = GridSearchCV(regressor, param_grid, n_jobs= -1, cv=5)
grid.fit(X_OC_train, y_OC_train)

print(grid.best_params_)

from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, explained_variance_score


# Calculate performance metrics
mse = mean_squared_error(y_OC_test, y_test_fit)

mae = mean_absolute_error(y_OC_test, y_test_fit)

# Print the performance metrics
print("Mean Squared Error (MSE):", mse)

print("Mean Absolute Error (MAE):", mae)

import numpy as np

# Calculate the range of the reference values
range_reference = np.max(y_OC_test) - np.min(y_OC_test)

# Calculate the differences between the predicted and reference values
differences = y_test_fit - y_OC_test

# Calculate the standard deviation of the differences
std_deviation = np.std(differences)

# Calculate the RPD
rpd = range_reference / std_deviation

# Print the RPD
print("RPD:", rpd)

import matplotlib.pyplot as plt
import seaborn as sns
# Create a scatter plot for the regression map
# plt.scatter(y_OC_test, nn.predict(X_OC_test))
# Create a scatter plot with a regression line
sns.regplot(x=y_OC_test, y=nn.predict(X_OC_test))

plt.xlabel("Actual Soil Organic Carbon")
plt.ylabel("Predicted Soil Organic Carbon")
plt.title("Regression Map - MLP Regressor")
plt.show()

